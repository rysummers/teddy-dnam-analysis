---
title: "T1D - Teddy Project"
author: "Ryan Summers"
date: "2026-01-06"
output:
  word_document: default
  pdf_document: default
  html_document:
    css: style.css
bibliography: t1d.bib
csl: "apa-5th-edition.csl"
nocite: '@*'
---

```{r setup, include=FALSE}
library(BiocManager)
library(knitr)
library(ggplot2)
library(tidyverse)
library(magrittr)
library(ggfortify)
library(olsrr)
library(kableExtra)
library(doBy)
library(MASS)
library(dplyr)
library(table1)
library(gtsummary)
library(gt)
library(stringdist)
library(ggrepel)
library(pwr)
library(ssize)
library(samr)
#library(simpleaffy)
library(parallel)
library(BiocParallel)
library(impute)
library(limma)
library(qvalue)
library(gtools)
library(multtest)
library(GEOquery)
library(Biobase)
library(data.table)
library(SummarizedExperiment)
library(variancePartition)
#library(IlluminaHumanMethylationEPICanno.ilm10b4.hg19)
#library(IlluminaHumanMethylationEPICv2anno.20a1.hg38)
library(minfi)
library(R.utils)
library(stringr)
#library(IlluminaHumanMethylationEPICmanifest)
library(methylKit)
library(shinyMethyl)
library(minfi)
library(bumphunter)
#library(IlluminaHumanMethylation450kmanifest)
#library(IlluminaHumanMethylation450kanno.ilmn12.hg19)
library(qs)
library(qs2)
library(lme4)
library(sesame)
library(sva)
library(EmpiricalBrownsMethod)
#library(TxDb.Hsapiens.UCSC.hg19.knownGene)
#library(org.Hs.eg.db)
library(clusterProfiler)
library(patchwork)
library(ggmanh)
library(survival)
library(lme4)
library(SummarizedExperiment)
library(variancePartition)
library(lmerTest)
library(future.apply)
library(data.table)
```

```{r}


BiocManager::install(c(
  "SummarizedExperiment",
  "BiocParallel",
  "variancePartition"))

```




```{r}
# TEMP for local testing
# setwd(paste0(Sys.getenv("RKJCOLLAB"), "/Maternal_Protection"))
# matrix_path <- "data/teddy/TEDDYarrays/preprocessing-dnam/mSet-norm-batch-mean-plate-row.Rdata"
# sample_ids_path <- "data/teddy/TEDDYarrays/samples-for-analysis.csv"
# pheno_path <- "data/teddy/TEDDYarrays/preprocessing-dnam/phenoData_v2.csv"
# model_path <- "/Users/slacksa/repos/explore_matprot/TEDDYcode/2-analysis/Functions/getSubSpecificInt_randomIntAndSlopeModel.R"
# # model_path <- "/Users/slacksa/repos/explore_matprot/TEDDYcode/2-analysis/Functions/getSubSpecificInt_randomIntModel.R"
# cell_types_path <- "data/teddy/TEDDYarrays/cell proportions/cell_counts_rescaled.csv"
# cov_list <- "factor(gender),new_CD8T,new_CD4T,new_NK,new_Bcell,new_Mono"
# # cov_list <- "factor(gender)"
# cov_list <- unlist(strsplit(cov_list, ","))
```


```{r}
# # load in M-value matrix
# M_read <- load("/Volumes/Storage/Maternal_Protection/data/teddy/TEDDYarrays/preprocessing-dnam/mSet-norm-batch-mean-plate-row.Rdata")
# 
# # save file using serialization - quicker
# qsave(mSet.norm.mean.plate_row, "/Volumes/Storage/Maternal_Protection/data/teddy/TEDDYarrays/preprocessing-dnam/M.matrix.qs", preset = "fast", nthreads = 6)
```


```{r}
# load in qs file
M.matrix <- qread("/Volumes/Storage/Maternal_Protection/data/teddy/TEDDYarrays/preprocessing-dnam/M.matrix.qs", nthreads = 8)

qsave(M.matrix, "/Users/ryan_summers/GitHub/teddy-dnam-analysis/data/raw/M.matrix.qs", nthreads = 6)
```


```{r}
# read in phenotype
pheno <- read.csv("/Volumes/Storage/Maternal_Protection/data/teddy/TEDDYarrays/preprocessing-dnam/phenoData_v2.csv")
```


```{r}
length(unique(pheno$maskid))
```


```{r}
# Load list of IDs to keep in analysis
sample_ids_df <- read.csv("/Volumes/Storage/Maternal_Protection/data/teddy/TEDDYarrays/samples-for-analysis.csv")
sample_ids <- sample_ids_df$rgName
```


```{r}
# Load cell type proportions
cell_types <- read.csv("/Users/ryan_summers/GitHub/teddy-dnam-analysis/data/processed/cell_counts_rescaled.csv")
```


```{r}
# Filter matrix to just samples for analysis
matrix.filt <- M.matrix[, which(colnames(M.matrix) %in% sample_ids)]

# save matrix file to local
qsave(matrix.filt, "/Users/ryan_summers/GitHub/teddy-dnam-analysis/data/processed/matrix.filt.qs", nthreads = 6)
```

```{r}
# load matrix
matrix.filt <- qread("/Users/ryan_summers/GitHub/teddy-dnam-analysis/data/processed/matrix.filt.qs", nthreads = 6)

# save into new qs2 package
qs_save(matrix.filt, "/Users/ryan_summers/GitHub/teddy-dnam-analysis/data/processed/matrix.filt.qs2",
        nthreads = 6)
```

```{r}
# Rename pheno to match function code & filter to just samples want in analysis
pheno_filt <- pheno %>%
  dplyr::filter(rgName %in% sample_ids)
```

```{r}
# Add cell types to pheno
  # TO NOTE: model should include all cell types except for neutrophils, doing
  # this essentially sets neutrophils as reference
pheno_filt_ct <- left_join(pheno_filt, cell_types,
                           by = c("Sample_Name" = "samplemaskid"))

# convert to years and center to help with different scale issue when modeling
pheno_filt_ct$age_yrs_c <-
  (pheno_filt_ct$sample_agedys / 365.25) -
  mean(pheno_filt_ct$sample_agedys / 365.25)

# scale the cell covariates
covars <- c("new_Bcell", "new_CD4T", "new_CD8T",
            "new_Mono", "new_NK")

pheno_filt_ct[covars] <- scale(pheno_filt_ct[covars])



# save to csv
write.csv(pheno_filt_ct, file = "/Users/ryan_summers/GitHub/teddy-dnam-analysis/data/processed/pheno_scaled.csv")
qsave(pheno_filt_ct, "/Users/ryan_summers/GitHub/teddy-dnam-analysis/data/processed/pheno_filt_ct.qs", nthreads = 6)
```


```{r}
# load in pheno file
pheno_filt_ct <- qread("/Users/ryan_summers/GitHub/teddy-dnam-analysis/data/processed/pheno_filt_ct.qs", nthreads = 6)
```



```{r}
pheno_filt_ct %>% 
  filter(maskid == 205235)
```


```{r}
# number of patients
length(unique(pheno_filt_ct$maskid))
```


```{r}
# measurement stats
pheno_filt_ct %>% 
  group_by(maskid) %>%
  summarise(n_measurements = n()) -> mnts_per_id 

mnts_per_id %>%   
  summarise(avg = mean(n_measurements),
            min = min(n_measurements),
            med = median(n_measurements),
            max = max(n_measurements))

hist(mnts_per_id$n_measurements, breaks = 12, freq = FALSE)
```

## Extract cg25271479 from matrix.filt and merge into phenotype

```{r}

# CpG of interest
cpg <- "cg25271479"

# confirm CpG exists in the matrix
stopifnot(cpg %in% rownames(matrix.filt))

# extract M-values for CpG across samples (named by rgName)
meth_vec <- matrix.filt[cpg, ]
stopifnot(all(names(meth_vec) == colnames(matrix.filt)))

# Merge into phenotype by rgName
pheno_df <- pheno_filt_ct %>%
  mutate(
    meth = meth_vec[rgName],
    age_days = sample_agedys,
    endpt_days = case_endptage)

# Check merge worked
cat("N rows:", nrow(pheno_df), "\n")
cat("N missing meth:", sum(is.na(pheno_df$meth)), "\n")
summary(pheno_df$meth)

```


## Sanity check: confirm 1:1 sets

```{r}
check_sets <- pheno_df %>%
  dplyr::distinct(case_ind, maskid, outcome, endpt_days) %>%
  group_by(case_ind) %>%
  summarise(
    n_ids = n_distinct(maskid),
    n_cases = sum(outcome == 1),
    n_ctrls = sum(outcome == 0),
    endpt_unique = n_distinct(endpt_days),
    .groups = "drop")

table(check_sets$n_ids)
table(check_sets$n_cases)
table(check_sets$endpt_unique)

bad_sets <- check_sets %>% 
  filter(n_ids != 2 | n_cases != 1 | endpt_unique != 1)
bad_sets

```



```{r}
# pick one CpG
probe1 <- rownames(matrix.filt)[6645]

res1 <- mod_function_fast(
  probe = "cg25271479",
  matrix = matrix.filt,
  pheno = pheno_filt_ct,
  sample_var = "rgName",   # <-- column names of matrix (rgName)
  id_var = "maskid",          
  age_var = "sample_agedys",  
  covs = c("gender", "cc"),   
  return_blups = FALSE)

res1
```
Eigenvalue: 
0.0345 - Substantial random-effects variability
5×10⁻⁹- Essentially zero variability

Interpretation
There is strong subject-to-subject variability in one direction (intercept), and almost none in the other (slope-related direction).

* Eigenvalues > 0 → matrix is positive definite (good)
* Very small eigenvalues → one direction has almost no variance
*	≈ 0 eigenvalue → model is near-singular
	
	

	
```{r}
library(future.apply)
library(data.table)

future::plan(future::multicore, workers = 6)   # adjust workers
options(future.globals.maxSize = 25 * 1024^3)  # 25GB for globals
```

A wrapper that calls your single-CpG function

This is where the “parallelization outside” happens: we create a function that fits one CpG, and future_lapply runs many instances of it in parallel.

```{r}
fit_one_cpg <- function(probe) {
  mod_function_fast(
    probe = probe,
    matrix = matrix.filt,
    pheno = pheno_filt_ct,
    sample_var = "rgName",    # <-- change
    id_var = "maskid",           # <-- change
    age_var = "sample_agedys",   # <-- change
    covs = c("gender", "cc"),    # <-- change
    return_blups = FALSE         # keep FALSE for speed
  )}

```

	
	
Run a small parallel test first	
```{r}
future::plan(future::multicore, workers = 6)
future::plan()
options(future.globals.maxSize = 20 * 1024^3)  # 20 GiBb

system.time({
  
test_probes <- rownames(matrix.filt)[1:39548] # dim = 790944x1407
test_res <- future_lapply(test_probes, fit_one_cpg)
test_res <- data.table::rbindlist(test_res, fill = TRUE)
print(test_res[1:3]) 

})

# save to csv
write.csv(test_res, file = "/Users/ryan_summers/GitHub/teddy-dnam-analysis/results/test_res.csv")

# load csv
test_res <- read_csv("/Users/ryan_summers/GitHub/teddy-dnam-analysis/results/test_res.csv")
```
    user   system  elapsed 
8063.489   73.480 8139.559 - 2.3hrs 

Estimated time to run all CpGs on my system:
20*8139.559 = 162791.2sec ≈ 2,714min ≈ 45.2hrs ≈ 2days


```{r}
# % of convergence issues based on 5% sample of 790,944 CpG sites
sum(!is.na(test_res$error_message)) / nrow(test_res) # 3.2%
```

```{r}
subset(test_res, CpG == "cg25271479")
dplyr::filter(test_res, CpG == "cg25271479")
test_res[test_res$CpG == "cg25271479", ]
```




Full run: chunk CpGs and write each chunk to disk (recommended)

With ~791k CpGs, do not store all results in RAM.
Below stores to disk to not overwhelm RAM
```{r}
probes <- rownames(matrix.filt)

chunk_size <- 2000
chunks <- split(probes, ceiling(seq_along(probes) / chunk_size))

out_dir <- "lme_chunks" # <----- SET DIRECTORY FOR LARGE FILES!!
dir.create(out_dir, showWarnings = FALSE)

chunk_files <- future_lapply(seq_along(chunks), function(k) {
  chunk <- chunks[[k]]

  res_list <- lapply(chunk, fit_one_cpg)
  res <- data.table::rbindlist(res_list, fill = TRUE)

  fn <- file.path(out_dir, sprintf("chunk_%04d.csv.gz", k))
  data.table::fwrite(res, fn)
  fn
})

# chunk_files is a vector of file paths (one per chunk)
length(chunk_files)
```


Later, you can read and combine only what you need:
```{r}
files <- list.files(out_dir, full.names = TRUE, pattern = "chunk_.*\\.csv\\.gz$")
res_all <- data.table::rbindlist(lapply(files, data.table::fread), fill = TRUE)
```

Optional: second pass for BLUPs only on selected CpGs

Once you filter CpGs from res_all (or chunk-by-chunk), compute BLUPs only for a manageable subset:
```{r}
top_cpgs <- res_all[fit_status != "error" & p_slope < 1e-4, CpG]
top_cpgs <- unique(top_cpgs)[1:200]

fit_one_cpg_blup <- function(probe) {
  mod_function_fast(
    probe = probe,
    matrix = matrix,
    pheno = pheno,
    sample_var = "sample_id",
    id_var = "maskid",
    age_var = "sample_agedys",
    covs = c("gender", "cc"),
    return_blups = TRUE
  )
}

blup_dir <- "blups_selected"
dir.create(blup_dir, showWarnings = FALSE)

blup_files <- future_lapply(top_cpgs, function(p) {
  out <- fit_one_cpg_blup(p)
  fn <- file.path(blup_dir, paste0(p, ".rds"))
  saveRDS(out, fn)
  fn
})
```



	
```{r}
m2beta <- function(m) {
  2^m / (1 + 2^m)}
```
	
```{r}
m2beta(0)
```




```{r}

res2 <- mod.function(
  probe = "cg25271479",
  matrix = matrix.filt,
  pheno = pheno_filt_ct,
  sample_var = "rgName",   
  id_var = "maskid",          
  age_var = "sample_agedys", 
  covs = c("gender", "cc"))

print(res2)
```


## Sanity check for custom functions

```{r}
# now fit new summarized experiment object w/ filtered CpGs
pheno_ordered <- pheno_filt_ct[match(colnames(matrix.filt), pheno_filt_ct$rgName), ]
stopifnot(all(!is.na(pheno_ordered$rgName)))

# test a small sample
set.seed(2522)
idx <- sample(nrow(matrix.filt), 5000)
matrix.sample <- matrix.filt[idx, , drop = F]

pheno_ordered <- pheno_filt_ct[match(colnames(matrix.sample), pheno_filt_ct$rgName), ]
stopifnot(all(!is.na(pheno_ordered$rgName)))

# build the SummarizedExperiment
se.filt <- SummarizedExperiment(
  assays = list(Mvalue = matrix.sample), #[1:39548,]
  colData = as.data.frame(pheno_ordered))

```


```{r}
# formula
form <- ~ age_yrs_c + gender + cc + new_Bcell + new_CD4T + new_CD8T + new_Mono + new_NK + (1 + age_yrs_c | maskid)
```

```{r}
# set up parallelization
n.cores <- parallel::detectCores() - 1
param <- SnowParam(5, type = "SOCK", progressbar = T)  
param <- MulticoreParam(workers = 5, progressbar = T)
```

```{r}
# fit the model - uses REML by default
system.time({
  dream.model2 <- suppressWarnings(
    dream(exprObj = assay(se.filt, "Mvalue"), 
          formula = form, 
          data = colData(se.filt), 
          ddf = "Satterthwaite", 
          BPPARAM = param))
  })

qsave(dream.model2, "/Users/ryan_summers/GitHub/teddy-dnam-analysis/results/dream.model2.qs", preset = "fast", nthreads = 6)
```

user   system  elapsed 
39.413   10.947 1176.774 - 19.6min

Est full:
≈ 392min ≈ 6.5hrs - 6.9x faster than custom function

```{r}
# view top results 
tt_T1DAge <- topTable(dream.model, coef = "diseaseCase:age_years",
         number = Inf, sort.by = "B") 

sig_tt <- topTable(dream.model, coef = "diseaseCase:age_years",
         number = Inf, sort.by = "B") 

nrow(subset(dream_test_res, adj.P.Val < 0.05))

# save to csv
dream_test_res <- topTable(dream.model2, number = Inf)
write.csv(dream_test_res, file = "/Users/ryan_summers/GitHub/teddy-dnam-analysis/results/dream_test_res.csv")
```

```{r}
# number of significant CpGs based on adjusted p-value
nrow(subset(dream_test_res, adj.P.Val < 0.05))
nrow(subset(dream_test_res, adj.P.Val < 0.05)) / nrow(matrix.filt[1:39548,])

```


```{r}
# Empirical Bayes - pulls info across all CpGs (stabilizes variances) 
dream.model.EB <- eBayes(dream.model)

# qs methods - soon to be deprecated
qsave(dream.model.EB, "dream.model.EB.qs", preset = "fast", nthreads = 6)


# load dream object
system.time({
  dream.model.EB <- qread("dream.model.EB.qs", nthreads = 8)
  })


```


Using Kenwood-Rogers to compare computational time an estimates

```{r}
# fit the model - uses REML by default
system.time({
  dream.model.KR <- suppressWarnings(
    dream(exprObj = assay(se.filt, "Mvalue"), 
          formula = form, 
          data = colData(se.filt), 
          ddf = "Kenward-Roger", 
          BPPARAM = param))
  })

qsave(dream.model.KR, "/Users/ryan_summers/GitHub/teddy-dnam-analysis/results/dream.model.KR.qs", preset = "fast", nthreads = 6)
```
    user   system  elapsed 
7136.401   72.690 1229.091 ~ 20.5min


```{r}
# save to csv
dream_test_res.KR <- topTable(dream.model.KR, number = Inf)
write.csv(dream_test_res.KR, file = "/Users/ryan_summers/GitHub/teddy-dnam-analysis/results/dream_test_res.KR.csv")
```

```{r}
nrow(subset(dream_test_res.KR, adj.P.Val < 0.05)) / nrow(matrix.filt[1:39548,])
```



## Test singularity issues from Dream for CpGs that gave issues in nlme pipeline

```{r}
# identify CpGs that didnt converge
# CpGs that threw an nlme error
bad_cpgs <- unique(test_res$CpG[!is.na(test_res$error_message)])

length(bad_cpgs)
head(bad_cpgs)

all_cpgs <- rownames(assay(se.filt, "Mvalue"))
bad_cpgs <- intersect(bad_cpgs, all_cpgs)

length(bad_cpgs)


```



```{r}

cpgs <- bad_cpgs

# 1) Precompute formula ONCE
form_cpg <- update(form, CpG ~ .)

# 2) Keep only columns used by the formula (big speed win)
vars_needed <- setdiff(all.vars(form_cpg), "CpG")
base_dat <- as.data.frame(colData(se.filt)[, vars_needed, drop = FALSE])

# 3) Pull the response matrix ONCE
Y <- assay(se.filt, "Mvalue")[cpgs, , drop = FALSE]

# 4) Chunk work to reduce Snow overhead
chunk_size <- 50
idx <- split(seq_along(cpgs), ceiling(seq_along(cpgs) / chunk_size))

res_list <- unlist(
  bplapply(
    idx,
    function(ii, Y, base_dat, form_cpg) {
      out <- vector("list", length(ii))
      for (k in seq_along(ii)) {
        i <- ii[k]
        dat <- base_dat
        dat$CpG <- as.numeric(Y[i, ])

        m <- tryCatch(
          suppressWarnings(
            lmer(form_cpg, data = dat, REML = TRUE)
            ),
          error = function(e) NULL
          )

        if (is.null(m)) {
          out[[k]] <- list(is_sing = NA, conv_msg = "lmer error")
        } else {
          msg <- m@optinfo$conv$lme4$messages
          out[[k]] <- list(
            is_sing  = lme4::isSingular(m, tol = 1e-4),
            conv_msg = if (length(msg)) paste(unlist(msg), collapse = " | ") else ""
          )
        }
      }
      out
    },
    Y = Y, base_dat = base_dat, form_cpg = form_cpg,
    BPPARAM = param
  ),
  recursive = FALSE
)

is_sing  <- vapply(res_list, `[[`, logical(1), "is_sing")
conv_msg <- vapply(res_list, `[[`, character(1), "conv_msg")

sing_df <- data.frame(CpG = cpgs, 
                      is_singular = is_sing, 
                      lmer_messages = conv_msg)

table(sing_df$is_singular, useNA = "ifany")

# write to csv
write.csv(sing_df, file = "/Users/ryan_summers/GitHub/teddy-dnam-analysis/results/sing_df.csv")
```


```{r}
sing_df <- data.frame(
  CpG = cpgs,
  is_singular = vapply(res_list, `[[`, logical(1), "is_sing"),
  lmer_messages = vapply(res_list, `[[`, character(1), "conv_msg"),
  stringsAsFactors = FALSE
)

table(sing_df$is_singular, useNA = "ifany")
head(subset(sing_df, is_singular))


```

“For each CpG we attempted a random-slope model. When fits were singular or showed convergence issues, we refit using a reduced random-effects structure (uncorrelated slope or random-intercept only). CpGs were excluded only if the reduced model failed or had insufficient observations.”





```{r}
cg25271479_df <- colData(se.filt) %>% 
  as.data.frame() %>%
  mutate(methylation = assay(se.filt, "Mvalue")["cg25271479", ])

## There is no exact, universally accepted denominator df because FE estimates
## depend on RE str and var components
## Therefore, lmer() omits p-values because df are ambiguous; in practice, lmerTest (Satterthwaite) is the standard way to obtain them, especially for large-scale longitudinal omics analyses.
cg25271479.lmer.model <- lmer(
  methylation ~ sample_agedys + gender + cc + (sample_agedys | maskid),
  data = cg25271479_df)

summary(cg25271479.lmer.model)

## The REML likelihood depends on the fixed‐effects design matrix
## REML integrates out fixed effects
## If you compare models with different fixed effects, the REML likelihoods are not comparable
ranova(cg25271479.lmer.model)

```

```{r}
isSingular(cg25271479.lmer.model, tol = getSingTol())
getSingTol()
```

How to see what is singular in your model
```{r}
VarCorr(cg25271479.lmer.model)
```


```{r}
cg25271479.lme.model <- lme(
  methylation ~ sample_agedys + gender + cc ,
  random = ~ sample_agedys | maskid,
  data = cg25271479_df)

summary(cg25271479.lme.model)
```





